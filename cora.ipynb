{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Planetoid(root='data/Cora', name='Cora')\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(citation_path, paper_path):\n",
    "\n",
    "    # Loading citation data\n",
    "    citations_data = pd.read_csv(citation_path,\n",
    "                                    sep=\"\\t\",\n",
    "                                    header=None,\n",
    "                                    names=[\"target\", \"source\"],\n",
    "                                    )\n",
    "    # Loading papers data\n",
    "    column_names = [\"paper_id\"] + [f\"term_{idx}\" for idx in range(1433)] + [\"subject\"]\n",
    "    papers_data = pd.read_csv(paper_path, \n",
    "                                sep=\"\\t\", \n",
    "                                header=None, \n",
    "                                names=column_names,)\n",
    "    papers_data = papers_data.sort_values('paper_id', ascending=True)\n",
    "    return papers_data, citations_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_mapping(papers_data, citations_data):\n",
    "\n",
    "    # Class mapping i\n",
    "    class_values = sorted(papers_data[\"subject\"].unique())\n",
    "    class_idc = {name: id for id, name in enumerate(class_values)}\n",
    "\n",
    "    # Paper Id mapping\n",
    "    paperid_values = sorted(papers_data[\"paper_id\"].unique())\n",
    "    paper_idc = {name: idx for idx, name in enumerate(paperid_values)}\n",
    "\n",
    "    papers_data[\"paper_id\"] = papers_data[\"paper_id\"].apply(lambda name: paper_idc[name])\n",
    "    citations_data[\"source\"] = citations_data[\"source\"].apply(lambda name: paper_idc[name])\n",
    "    citations_data[\"target\"] = citations_data[\"target\"].apply(lambda name: paper_idc[name])\n",
    "    papers_data[\"subject\"] = papers_data[\"subject\"].apply(lambda value: class_idc[value])\n",
    "\n",
    "    mappings = (class_idc, paper_idc)\n",
    "\n",
    "    return papers_data, citations_data, mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(papers_data, citations_data):\n",
    "    \n",
    "    # get node feature names\n",
    "    feature_names = set(papers_data.columns) - {\"paper_id\", \"subject\"}\n",
    "\n",
    "    # create edges array [2, num_edges].\n",
    "    edges = citations_data[[\"source\", \"target\"]].to_numpy().T\n",
    "    edge_index = torch.from_numpy(edges).to(torch.long)\n",
    "\n",
    "    # create node features array [num_nodes, num_features].\n",
    "    node_features = papers_data.sort_values(\"paper_id\")[feature_names].to_numpy()\n",
    "    node_features = torch.from_numpy(node_features).type(torch.FloatTensor)\n",
    "    labels = torch.from_numpy(papers_data[\"subject\"].values).to(torch.long)\n",
    "\n",
    "    # create graph data\n",
    "    data = Data(x=node_features, edge_index = edge_index, y=labels)\n",
    "\n",
    "    # print(\"Edges shape:\", edges.shape)\n",
    "    #print(\"Nodes shape:\", node_features.shape)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_feats, hidden_channels, out_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        # Initialize the layers\n",
    "        self.conv1 = GCNConv(input_feats, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.out = Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # First Message Passing Layer (Transformation)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.4, training=self.training)\n",
    "\n",
    "        # Second Message Passing Layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.4, training=self.training)\n",
    "\n",
    "        # Output layer \n",
    "        x = F.log_softmax(self.out(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(utils, new_data):\n",
    "\n",
    "      model, optimizer, criterion = utils\n",
    "\n",
    "      model.train()\n",
    "      optimizer.zero_grad() \n",
    "      # Use all data as input, because all nodes have node features\n",
    "      out = model(new_data.x, new_data.edge_index)\n",
    "      \n",
    "      pred = out.argmax(dim=1)  \n",
    "      test_correct = (pred[data.train_mask] == new_data.y[data.train_mask])\n",
    "      acc = int(test_correct.sum()) / int(data.train_mask.sum())\n",
    "     \n",
    "      # Only use nodes with labels available for loss calculation --> mask\n",
    "      loss = criterion(out[data.train_mask], new_data.y[data.train_mask]) \n",
    "      loss.backward() \n",
    "      optimizer.step()\n",
    "      return loss, acc\n",
    "\n",
    "def test(model):\n",
    "      model.eval()\n",
    "      out = model(new_data.x, new_data.edge_index)\n",
    "      # Use the class with highest probability.\n",
    "      pred = out.argmax(dim=1)  \n",
    "      # Check against ground-truth labels.\n",
    "      test_correct = (pred[data.test_mask] == new_data.y[data.test_mask])  \n",
    "      # Derive ratio of correct predictions.\n",
    "      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  \n",
    "      return test_acc\n",
    "\n",
    "def val(model):\n",
    "      model.eval()\n",
    "      out = model(new_data.x, new_data.edge_index)\n",
    "      # Use the class with highest probability.\n",
    "      pred = out.argmax(dim=1)  \n",
    "      # Check against ground-truth labels.\n",
    "      test_correct = (pred[data.val_mask] == new_data.y[data.val_mask])  \n",
    "      # Derive ratio of correct predictions.\n",
    "      test_acc = int(test_correct.sum()) / int(data.val_mask.sum())  \n",
    "      return test_acc\n",
    "\n",
    "def get_keys(d, value):\n",
    "    \n",
    "    for k, v in d.items():\n",
    "        if v == value:\n",
    "            return k\n",
    "\n",
    "def new_input(new_node, citation):\n",
    "\n",
    "    x = torch.cat((new_data.x, new_node), dim = 0)\n",
    "    context_edges = citation.to_numpy()\n",
    "    context_edges = torch.from_numpy(context_citation).to(torch.long).T\n",
    "    x_index = torch.cat((new_data.edge_index, context_edges), dim = 1)\n",
    "    return x, x_index\n",
    "\n",
    "def infrence(papers_data):\n",
    "    train_data, test_data = [], []\n",
    "    for _, group in papers_data.groupby(\"subject\"):\n",
    "        # Select around 50% of the dataset for training.\n",
    "        random_selection = np.random.rand(len(group.index)) <= 0.8\n",
    "        train_data.append(group[random_selection])\n",
    "        test_data.append(group[~random_selection])\n",
    "\n",
    "    train_data = pd.concat(train_data).sample(frac=1)\n",
    "    test_data = pd.concat(test_data).sample(frac=1)\n",
    "\n",
    "    # get node feature names\n",
    "    feature_names = set(papers_data.columns) - {\"paper_id\", \"subject\"}\n",
    "\n",
    "    # create node features array [num_nodes, num_features].\n",
    "    node_features = test_data[feature_names].to_numpy()\n",
    "    node_features = torch.from_numpy(node_features).type(torch.FloatTensor)\n",
    "    return node_features,test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "citation_path = './cora.cites'\n",
    "paper_path = './cora.content'\n",
    "\n",
    "papers_data, citations_data = data_prep(citation_path, paper_path)\n",
    "papers_data, citations_data, mappings = value_mapping(papers_data, citations_data)\n",
    "new_data = extract_features(papers_data, citations_data)\n",
    "class_idc, paper_idc = mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRD(torch.nn.Module):\n",
    "    def __init__(self, d_in, d_out, p):\n",
    "        super(CRD, self).__init__()\n",
    "        self.conv = GCNConv(d_in, d_out, cached=True) \n",
    "        self.p = p\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index, mask=None):\n",
    "        x = F.relu(self.conv(x, edge_index))\n",
    "        x = F.dropout(x, p=self.p, training=self.training)\n",
    "        return x\n",
    "\n",
    "class CLS(torch.nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super(CLS, self).__init__()\n",
    "        self.conv = GCNConv(d_in, d_out, cached=True)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index, mask=None):\n",
    "        x = self.conv(x, edge_index)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.crd = CRD(1433, 64, 0.5)\n",
    "        self.cls = CLS(64, 7)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.crd.reset_parameters()\n",
    "        self.cls.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.crd(x, edge_index)\n",
    "        x = self.cls(x, edge_index)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "input_feats = new_data.x.shape[1]\n",
    "hidden_channels = 64\n",
    "out_channels = 7\n",
    "\n",
    "model = GCN(input_feats, hidden_channels, out_channels)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # Use GPU\n",
    "model = model.to(device)\n",
    "new_data = new_data.to(device)\n",
    "\n",
    "# Initialize Optimizer\n",
    "learning_rate = 0.01\n",
    "decay = 5e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), \n",
    "                             lr=learning_rate, \n",
    "                             weight_decay=decay)\n",
    "\n",
    "# Define loss function (CrossEntropyLoss for Classification Problems with \n",
    "# probability distributions)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "utils = (model, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Loss: 1.9744 Accuracy: 0.1000\n",
      "Epoch: 100, Loss: 0.0284 Accuracy: 0.9857\n",
      "Epoch: 200, Loss: 0.0068 Accuracy: 1.0000\n",
      "Epoch: 300, Loss: 0.0019 Accuracy: 1.0000\n",
      "Epoch: 400, Loss: 0.0080 Accuracy: 1.0000\n",
      "Epoch: 500, Loss: 0.0266 Accuracy: 0.9857\n",
      "Epoch: 600, Loss: 0.0033 Accuracy: 1.0000\n",
      "Epoch: 700, Loss: 0.0027 Accuracy: 1.0000\n",
      "Epoch: 800, Loss: 0.0019 Accuracy: 1.0000\n",
      "Epoch: 900, Loss: 0.0090 Accuracy: 1.0000\n",
      "Epoch: 1000, Loss: 0.0059 Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for epoch in range(0, 1001):\n",
    "    loss, acc = train(utils, new_data)\n",
    "    losses.append(loss)\n",
    "    if epoch % 100 == 0:\n",
    "      print(f'Epoch: {epoch:03d}, Loss: {loss:.4f} Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5600\n",
      "Validation Accuracy: 0.5920\n"
     ]
    }
   ],
   "source": [
    "test_acc = test(model)\n",
    "print(f'Test Accuracy: {test_acc:.4f}')\n",
    "\n",
    "val_acc = val(model)\n",
    "print(f'Validation Accuracy: {val_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "node_features, test_data = infrence(papers_data)\n",
    "\n",
    "new_lab = []\n",
    "for i in range(node_features.shape[0]):\n",
    "    \n",
    "    new_data_feats = node_features[i].unsqueeze(0)\n",
    "    x = torch.cat((new_data.x, new_data_feats), dim = 0)\n",
    "    out = model(x,new_data.edge_index)\n",
    "    pred = out.argmax(dim=1)  \n",
    "    new_lab.append(pred[-1])\n",
    "    #print(f'{get_keys(class_idc, pred[-1])}, its index is {pred[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5535390199637024"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab = test_data['subject'].values.tolist()\n",
    "new_lab, lab = np.array(new_lab), np.array(lab)\n",
    "corr = (new_lab == lab).sum()\n",
    "corr/node_features.shape[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
