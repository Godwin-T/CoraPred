{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Planetoid(root='data/Cora', name='Cora')\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(citation_path, paper_path):\n",
    "\n",
    "    # Loading citation data\n",
    "    citations_data = pd.read_csv(citation_path,\n",
    "                                    sep=\"\\t\",\n",
    "                                    header=None,\n",
    "                                    names=[\"target\", \"source\"],\n",
    "                                    )\n",
    "    # Loading papers data\n",
    "    column_names = [\"paper_id\"] + [f\"term_{idx}\" for idx in range(1433)] + [\"subject\"]\n",
    "    papers_data = pd.read_csv(paper_path, \n",
    "                                sep=\"\\t\", \n",
    "                                header=None, \n",
    "                                names=column_names,)\n",
    "    papers_data = papers_data.sort_values('paper_id', ascending=True)\n",
    "    return papers_data, citations_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_mapping(papers_data, citations_data):\n",
    "\n",
    "    # Class mapping i\n",
    "    class_values = sorted(papers_data[\"subject\"].unique())\n",
    "    class_idc = {name: id for id, name in enumerate(class_values)}\n",
    "\n",
    "    # Paper Id mapping\n",
    "    paperid_values = sorted(papers_data[\"paper_id\"].unique())\n",
    "    paper_idc = {name: idx for idx, name in enumerate(paperid_values)}\n",
    "\n",
    "    papers_data[\"paper_id\"] = papers_data[\"paper_id\"].apply(lambda name: paper_idc[name])\n",
    "    citations_data[\"source\"] = citations_data[\"source\"].apply(lambda name: paper_idc[name])\n",
    "    citations_data[\"target\"] = citations_data[\"target\"].apply(lambda name: paper_idc[name])\n",
    "    papers_data[\"subject\"] = papers_data[\"subject\"].apply(lambda value: class_idc[value])\n",
    "\n",
    "    mappings = (class_idc, paper_idc)\n",
    "\n",
    "    return papers_data, citations_data, mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(papers_data, citations_data):\n",
    "    \n",
    "    # get node feature names\n",
    "    feature_names = set(papers_data.columns) - {\"paper_id\", \"subject\"}\n",
    "\n",
    "    # create edges array [2, num_edges].\n",
    "    st_edges = citations_data[[\"source\", \"target\"]].to_numpy().T\n",
    "    ts_edges = citations_data[[\"target\", \"source\"]].to_numpy().T\n",
    "    edges = np.concatenate([st_edges, ts_edges], axis=1)\n",
    "    edge_index = torch.from_numpy(edges).to(torch.long)\n",
    "\n",
    "    # create node features array [num_nodes, num_features].\n",
    "    node_features = papers_data.sort_values(\"paper_id\")[feature_names].to_numpy()\n",
    "    node_features = torch.from_numpy(node_features).type(torch.FloatTensor)\n",
    "    labels = torch.from_numpy(papers_data[\"subject\"].values).to(torch.long)\n",
    "\n",
    "    # create graph data\n",
    "    data = Data(x=node_features, edge_index = edge_index, y=labels)\n",
    "\n",
    "    # print(\"Edges shape:\", edges.shape)\n",
    "    #print(\"Nodes shape:\", node_features.shape)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_feats, hidden_channels, out_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        # Initialize the layers\n",
    "        self.conv1 = GCNConv(input_feats, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.out = Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # First Message Passing Layer (Transformation)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.4, training=self.training)\n",
    "\n",
    "        # Second Message Passing Layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.4, training=self.training)\n",
    "\n",
    "        # Output layer \n",
    "        x = F.log_softmax(self.out(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(utils, new_data):\n",
    "\n",
    "      model, optimizer, criterion = utils\n",
    "\n",
    "      model.train()\n",
    "      optimizer.zero_grad() \n",
    "      # Use all data as input, because all nodes have node features\n",
    "      out = model(new_data.x, new_data.edge_index)\n",
    "      \n",
    "      pred = out.argmax(dim=1)  \n",
    "      test_correct = (pred[data.train_mask] == new_data.y[data.train_mask])\n",
    "      acc = int(test_correct.sum()) / int(data.train_mask.sum())\n",
    "     \n",
    "      # Only use nodes with labels available for loss calculation --> mask\n",
    "      loss = criterion(out[data.train_mask], new_data.y[data.train_mask]) \n",
    "      loss.backward() \n",
    "      optimizer.step()\n",
    "      return loss, acc\n",
    "\n",
    "def test(model, new_data):\n",
    "      model.eval()\n",
    "      out = model(new_data.x, new_data.edge_index)\n",
    "      # Use the class with highest probability.\n",
    "      pred = out.argmax(dim=1)  \n",
    "      # Check against ground-truth labels.\n",
    "      test_correct = (pred[data.test_mask] == new_data.y[data.test_mask])  \n",
    "      # Derive ratio of correct predictions.\n",
    "      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  \n",
    "      return test_acc\n",
    "\n",
    "def val(model, new_data):\n",
    "      model.eval()\n",
    "      out = model(new_data.x, new_data.edge_index)\n",
    "      # Use the class with highest probability.\n",
    "      pred = out.argmax(dim=1)  \n",
    "      # Check against ground-truth labels.\n",
    "      test_correct = (pred[data.val_mask] == new_data.y[data.val_mask])  \n",
    "      # Derive ratio of correct predictions.\n",
    "      test_acc = int(test_correct.sum()) / int(data.val_mask.sum())  \n",
    "      return test_acc\n",
    "\n",
    "def get_keys(d, value):\n",
    "    \n",
    "    for k, v in d.items():\n",
    "        if v == value:\n",
    "            return k\n",
    "\n",
    "def new_input(new_node, citation, new_data, ):\n",
    "\n",
    "    x = torch.cat((new_data.x, new_node), dim = 0)\n",
    "    context_edges = citation.to_numpy()\n",
    "    context_edges = torch.from_numpy(new_citation).to(torch.long).T\n",
    "    x_index = torch.cat((new_data.edge_index, context_edges), dim = 1)\n",
    "    return x, x_index\n",
    "\n",
    "def infrence(papers_data):\n",
    "    train_data, test_data = [], []\n",
    "    for _, group in papers_data.groupby(\"subject\"):\n",
    "        # Select around 50% of the dataset for training.\n",
    "        random_selection = np.random.rand(len(group.index)) <= 0.8\n",
    "        train_data.append(group[random_selection])\n",
    "        test_data.append(group[~random_selection])\n",
    "\n",
    "    train_data = pd.concat(train_data).sample(frac=1)\n",
    "    test_data = pd.concat(test_data).sample(frac=1)\n",
    "\n",
    "    # get node feature names\n",
    "    feature_names = set(papers_data.columns) - {\"paper_id\", \"subject\"}\n",
    "\n",
    "    # create node features array [num_nodes, num_features].\n",
    "    node_features = test_data[feature_names].to_numpy()\n",
    "    node_features = torch.from_numpy(node_features).type(torch.FloatTensor)\n",
    "    return node_features,test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "citation_path = './cora.cites'\n",
    "paper_path = './cora.content'\n",
    "input_feats = 1433 \n",
    "hidden_channels = 16\n",
    "out_channels = 7\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "learning_rate = 0.01\n",
    "decay = 5e-4\n",
    "\n",
    "papers_data, citations_data = data_prep(citation_path, paper_path)\n",
    "papers_data, citations_data, mappings = value_mapping(papers_data, citations_data)\n",
    "new_data = extract_features(papers_data, citations_data)\n",
    "class_idc, paper_idc = mappings\n",
    "\n",
    "\n",
    "model = GCN(input_feats, hidden_channels, out_channels)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # Use GPU\n",
    "model = model.to(device)\n",
    "new_data = new_data.to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), \n",
    "                            lr=learning_rate, \n",
    "                            weight_decay=decay)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "utils = (model, optimizer, criterion)\n",
    "\n",
    "# losses = []\n",
    "# for epoch in range(0, epochs):\n",
    "#     loss, acc = train(utils, new_data)\n",
    "#     losses.append(loss)\n",
    "\n",
    "# test_acc = test(model, new_data)\n",
    "# print(f'Test Accuracy: {test_acc:.4f}')\n",
    "\n",
    "# val_acc = val(model, new_data)\n",
    "# print(f'Validation Accuracy: {val_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "citation_path = './cora.cites'\n",
    "paper_path = './cora.content'\n",
    "# Initialize model\n",
    "input_feats = 1433 #new_data.x.shape[1]\n",
    "out_channels = 7\n",
    "\n",
    "# Initialize Optimizer\n",
    "learning_rate = 0.01\n",
    "decay = 5e-4\n",
    "\n",
    "def final_train(citation_path, paper_path, \n",
    "                input_feats = 1433, hidden_channels = 16,\n",
    "                out_channels = 7, learning_rate = 0.01,\n",
    "                decay = 5e-4, epochs = 1001):\n",
    "\n",
    "    papers_data, citations_data = data_prep(citation_path, paper_path)\n",
    "    papers_data, citations_data, mappings = value_mapping(papers_data, citations_data)\n",
    "    new_data = extract_features(papers_data, citations_data)\n",
    "    class_idc, paper_idc = mappings\n",
    "\n",
    "    \n",
    "\n",
    "    model = GCN(input_feats, hidden_channels, out_channels)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # Use GPU\n",
    "    model = model.to(device)\n",
    "    new_data = new_data.to(device)\n",
    "\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), \n",
    "                                lr=learning_rate, \n",
    "                                weight_decay=decay)\n",
    "\n",
    "    # Define loss function (CrossEntropyLoss for Classification Problems with \n",
    "    # probability distributions)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    utils = (model, optimizer, criterion)\n",
    "\n",
    "    losses = []\n",
    "    for epoch in range(0, epochs):\n",
    "        loss, acc = train(utils, new_data)\n",
    "        losses.append(loss)\n",
    "        #if epoch % 100 == 0:\n",
    "            #print(f'Epoch: {epoch:03d}, Loss: {loss:.4f} Accuracy: {acc:.4f}')\n",
    "\n",
    "    test_acc = test(model, new_data)\n",
    "    print(f'Test Accuracy: {test_acc:.4f}')\n",
    "\n",
    "    val_acc = val(model, new_data)\n",
    "    print(f'Validation Accuracy: {val_acc:.4f}')\n",
    "    return model, papers_data, citations_data, new_data, class_idc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infrence(papers_data):\n",
    "    ''''For making infrence on new data'''\n",
    "    # train_data, test_data = [], []\n",
    "    # for _, group in papers_data.groupby(\"subject\"):\n",
    "    #     # Select around 50% of the dataset for training.\n",
    "    #     random_selection = np.random.rand(len(group.index)) <= 0.8\n",
    "    #     train_data.append(group[random_selection])\n",
    "    #     test_data.append(group[~random_selection])\n",
    "\n",
    "    # train_data = pd.concat(train_data).sample(frac=1)\n",
    "    # test_data = pd.concat(test_data).sample(frac=1)\n",
    "\n",
    "    # get node feature names\n",
    "    feature_names = set(papers_data.columns) - {\"paper_id\", \"subject\"}\n",
    "\n",
    "    # create node features array [num_nodes, num_features].\n",
    "    node_features = papers_data[feature_names].to_numpy()\n",
    "    node_features = torch.from_numpy(node_features).type(torch.FloatTensor)\n",
    "    return node_features,papers_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7350\n",
      "Validation Accuracy: 0.6900\n"
     ]
    }
   ],
   "source": [
    "model, papers_data, citation_data, new_data, class_idc = final_train(citation_path, paper_path)\n",
    "\n",
    "feature_names = set(papers_data.columns) - {\"paper_id\", \"subject\"}\n",
    "lab = papers_data['subject'].values.tolist()\n",
    "\n",
    "# create node features array [num_nodes, num_features].\n",
    "node_features = papers_data[feature_names].to_numpy()\n",
    "node_features = torch.from_numpy(node_features).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.587149187592319"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "new_lab = []\n",
    "for i in range(node_features.shape[0]):\n",
    "    \n",
    "    new_data_feats = node_features[i].unsqueeze(0)\n",
    "\n",
    "    target_ = citation_data[citation_data['target']==i] \n",
    "    edge = torch.from_numpy(target_.to_numpy()).T\n",
    "\n",
    "    x = torch.cat((new_data.x, new_data_feats), dim = 0)\n",
    "    new_edge = torch.cat((new_data.edge_index, edge), dim = 1)\n",
    "\n",
    "    out = model(x,new_edge)\n",
    "    pred = out.argmax(dim=1)  \n",
    "    new_lab.append(pred[-1])\n",
    "\n",
    "\n",
    "new_lab, lab = np.array(new_lab), np.array(lab)\n",
    "corr = (new_lab == lab).sum()\n",
    "corr/node_features.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_prediction(model, graph_data, citation_data,  data):\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    feature_names = set(data.columns) - {\"paper_id\", \"subject\"}\n",
    "\n",
    "    # create node features array [num_nodes, num_features].\n",
    "    node_features = data[feature_names].to_numpy()\n",
    "    node_features = torch.from_numpy(node_features).type(torch.FloatTensor)\n",
    "\n",
    "    new_lab = []\n",
    "    for i in range(node_features.shape[0]):\n",
    "\n",
    "        new_data_feats = node_features[i].unsqueeze(0)\n",
    "\n",
    "        target_ = citation_data[citation_data['target']==i] \n",
    "        edge = torch.from_numpy(target_.to_numpy()).T\n",
    "\n",
    "        x = torch.cat((graph_data.x, new_data_feats), dim = 0)\n",
    "        new_edge = torch.cat((graph_data.edge_index, edge), dim = 1)\n",
    "\n",
    "        out = model(x,new_edge)\n",
    "        pred = out.argmax(dim=1)  \n",
    "        new_lab.append(pred[-1])\n",
    "    return new_lab\n",
    "\n",
    "\n",
    "def single_prediction(model, graph_data, data, new_edges):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    node_features = torch.from_numpy(data).type(torch.FloatTensor)\n",
    "    node_features = node_features.unsqueeze(0)\n",
    "\n",
    "    x = torch.cat((graph_data.x, node_features), dim = 0)\n",
    "    new_edge = torch.cat((graph_data.edge_index, new_edges), dim = 1)\n",
    "\n",
    "    out = model(x,new_edge)\n",
    "    pred = out.argmax(dim=1)[-1]\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_e = citation_data[citation_data['target']==10] \n",
    "new_edges = torch.from_numpy(target_e.to_numpy()).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
